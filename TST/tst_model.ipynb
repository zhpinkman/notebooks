{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "200598d5-a9e7-46d0-87fc-fac0743288f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch import nn, Tensor\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "57f7cf69-9684-4fe8-bef8-71d100942ef6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoder(nn.Module):\n",
    "    \"\"\"\n",
    "    Adapted from: \n",
    "    https://pytorch.org/tutorials/beginner/transformer_tutorial.html\n",
    "    https://github.com/LiamMaclean216/Pytorch-Transfomer/blob/master/utils.py \n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, dropout: float = 0.1, max_seq_len: int = 5000, d_model: int = 512):\n",
    "\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            dropout: the dropout rate\n",
    "            max_seq_len: the maximum length of the input sequences\n",
    "            d_model: The dimension of the output of sub-layers in the model \n",
    "                     (Vaswani et al, 2017)\n",
    "        \"\"\"\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "        self.d_model = d_model\n",
    "        \n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        # Create constant positional encoding matrix with values \n",
    "        # dependent on position and i\n",
    "        position = torch.arange(max_seq_len).unsqueeze(1)\n",
    "        \n",
    "        exp_input = torch.arange(0, d_model, 2) * (-math.log(10000.0) / d_model)\n",
    "        \n",
    "        div_term = torch.exp(exp_input) # Returns a new tensor with the exponential of the elements of exp_input\n",
    "        \n",
    "        pe = torch.zeros(max_seq_len, d_model)\n",
    "\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        \n",
    "        pe[:, 1::2] = torch.cos(position * div_term) # torch.Size([target_seq_len, dim_val])\n",
    "\n",
    "        pe = pe.unsqueeze(0).transpose(0, 1) # torch.Size([target_seq_len, input_size, dim_val])\n",
    "\n",
    "        # register that pe is not a model parameter\n",
    "        self.register_buffer('pe', pe)\n",
    "        \n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: Tensor, shape [batch_size, enc_seq_len, dim_val]\n",
    "        \"\"\"\n",
    "\n",
    "        add = self.pe[:x.size(1), :].squeeze(1)\n",
    "\n",
    "        x = x + add\n",
    "\n",
    "        return self.dropout(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3e648525-fd6a-44a6-b238-f425d4285835",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TimeSeriesTransformer(nn.Module):\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        input_size, \n",
    "        dec_seq_len,\n",
    "        out_seq_len,\n",
    "        max_seq_len,\n",
    "        dim_val,\n",
    "        n_encoder_layers = 4,\n",
    "        n_decoder_layers = 4,\n",
    "        dropout_encoder: float=0.2, \n",
    "        dropout_decoder: float=0.2,\n",
    "        dropout_pos_enc: float=0.2,\n",
    "        dim_feedforward_encoder: int=2048,\n",
    "        dim_feedforward_decoder: int=2048,\n",
    "        n_heads = 8,\n",
    "    ):\n",
    "        \n",
    "        super().__init__()\n",
    "        self.dec_seq_len = dec_seq_len\n",
    "\n",
    "        print(\"input_size is: {}\".format(input_size))\n",
    "        print(\"dim_val is: {}\".format(dim_val))\n",
    "        \n",
    "        self.encoder_input_layers = nn.Linear(in_features=input_size, out_features=dim_val)\n",
    "        self.decoder_input_layer = nn.Linear(in_features=input_size, out_features=dim_val)\n",
    "        \n",
    "        \n",
    "        self.positional_encoder = PositionalEncoder(dropout=dropout_pos_enc, max_seq_len=max_seq_len, d_model=dim_val)\n",
    "        \n",
    "        encoder_layers = nn.TransformerEncoderLayer(d_model=dim_val, nhead=n_heads, dropout=dropout_encoder, dim_feedforward = dim_feedforward_encoder)\n",
    "        self.encoder = nn.TransformerEncoder(encoder_layer=encoder_layers, num_layers=n_encoder_layers)\n",
    "        \n",
    "        decoder_layer = nn.TransformerDecoderLayer(d_model=dim_val, nhead=n_heads, dropout=dropout_decoder, dim_feedforward = dim_feedforward_decoder)\n",
    "        self.decoder = nn.TransformerDecoder(decoder_layer=decoder_layer, num_layers=n_decoder_layers)\n",
    "        \n",
    "        self.decoder_linear_mapping = nn.Linear(in_features=out_seq_len * dim_val, out_features=out_seq_len)\n",
    "        \n",
    "        \n",
    "        \n",
    "    def forward(self, src, trg, src_mask, trg_mask):\n",
    "        \n",
    "        src = self.encoder_input_layers(src)\n",
    "        src = self.positional_encoder(src)\n",
    "        \n",
    "        src = self.encoder(src = src)\n",
    "        \n",
    "        \n",
    "        trg = self.decoder_input_layer(trg)\n",
    "        trg = self.decoder(trg = trg, trg_mask = trg_mask, memory = src, memory_mask = src_mask)\n",
    "        \n",
    "        \n",
    "        decoder_output = self.decoder_linear_mapping(trg)\n",
    "        return decoder_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "19e182c4-b370-4fb5-a59f-55dbf0ee78c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_size is: 1\n",
      "dim_val is: 512\n"
     ]
    }
   ],
   "source": [
    "input_size = 1 # The number of features we want to use\n",
    "\n",
    "dec_seq_len = 30\n",
    "enc_seq_len = 150\n",
    "out_seq_len = 1\n",
    "\n",
    "dim_val = 512\n",
    "\n",
    "max_seq_len = enc_seq_len\n",
    "\n",
    "\n",
    "\n",
    "model = TimeSeriesTransformer(\n",
    "    dec_seq_len=dec_seq_len,\n",
    "    out_seq_len=out_seq_len,\n",
    "    dim_val=dim_val,\n",
    "    max_seq_len = max_seq_len,\n",
    "    input_size = input_size, \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "48b0fb61-e032-45bb-ad34-c395e81e890f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_src_trg(\n",
    "        sequence: torch.Tensor, \n",
    "        enc_seq_len: int, \n",
    "        dec_seq_len: int, \n",
    "        target_seq_len: int\n",
    "    ):\n",
    "\n",
    "    assert len(sequence) == enc_seq_len + target_seq_len, \"Sequence length does not equal (input length + target length)\"\n",
    "\n",
    "    # encoder input\n",
    "    src = sequence[:enc_seq_len] \n",
    "\n",
    "    # decoder input. As per the paper, it must have the same dimension as the \n",
    "    # target sequence, and it must contain the last value of src, and all\n",
    "    # values of trg_y except the last (i.e. it must be shifted right by 1)\n",
    "    trg = sequence[enc_seq_len-1:len(sequence)-1]\n",
    "\n",
    "    assert len(trg) == target_seq_len, \"Length of trg does not match target sequence length\"\n",
    "\n",
    "    # The target sequence against which the model output will be compared to compute loss\n",
    "    trg_y = sequence[-target_seq_len:]\n",
    "\n",
    "    assert len(trg_y) == target_seq_len, \"Length of trg_y does not match target sequence length\"\n",
    "\n",
    "    return src, trg, trg_y.squeeze(-1) # change size from [batch_size, target_seq_len, num_features] to [batch_size, target_seq_len] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "465b34e3-a905-44e7-8635-56dd670010f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_square_mask(dim1, dim2, dim3):\n",
    "    \"\"\"\n",
    "    dim1: int, batch_size * n_heads\n",
    "    dim2: int, length of the input sequence \n",
    "    dim3: int, length of the encoder sequence length\n",
    "    \"\"\"\n",
    "    return torch.triu(torch.ones(dim1, dim2, dim3) * float('-inf'), diagonal=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a008a6ea-cd37-42f8-94c0-1561dafc8b25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Input length\n",
    "# enc_seq_len = 100\n",
    "\n",
    "# # Output length\n",
    "# output_sequence_length = 58\n",
    "\n",
    "# # Heads in attention layers\n",
    "# n_heads = 8\n",
    "\n",
    "# batch_size = 32\n",
    "\n",
    "# # Make src mask for decoder with size:\n",
    "# # [batch_size*n_heads, output_sequence_length, enc_seq_len]\n",
    "# src_mask = generate_square_mask(\n",
    "#     dim1=batch_size*n_heads,\n",
    "#     dim2=output_sequence_length,\n",
    "#     dim3=enc_seq_len\n",
    "#     )\n",
    "\n",
    "# # Make tgt mask for decoder with size:\n",
    "# # [batch_size*n_heads, output_sequence_length, output_sequence_length]\n",
    "# tgt_mask = generate_square_mask( \n",
    "#     dim1=batch_size*n_heads,\n",
    "#     dim2=output_sequence_length,\n",
    "#     dim3=output_sequence_length\n",
    "#     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cec64960-26a2-4b3d-bdbc-e4f1112bc906",
   "metadata": {},
   "outputs": [],
   "source": [
    "# output = model(\n",
    "#     src=src, \n",
    "#     tgt=trg,\n",
    "#     src_mask=src_mask,\n",
    "#     tgt_mask=tgt_mask\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4813697d-56a6-4f56-a1c2-31306f3fef9b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:general]",
   "language": "python",
   "name": "conda-env-general-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
