{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "200598d5-a9e7-46d0-87fc-fac0743288f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch import nn, Tensor\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "57f7cf69-9684-4fe8-bef8-71d100942ef6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoder(nn.Module):\n",
    "    \"\"\"\n",
    "    Adapted from: \n",
    "    https://pytorch.org/tutorials/beginner/transformer_tutorial.html\n",
    "    https://github.com/LiamMaclean216/Pytorch-Transfomer/blob/master/utils.py \n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, dropout: float = 0.1, max_seq_len: int = 5000, d_model: int = 512):\n",
    "\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            dropout: the dropout rate\n",
    "            max_seq_len: the maximum length of the input sequences\n",
    "            d_model: The dimension of the output of sub-layers in the model \n",
    "                     (Vaswani et al, 2017)\n",
    "        \"\"\"\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "        self.d_model = d_model\n",
    "        \n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        # Create constant positional encoding matrix with values \n",
    "        # dependent on position and i\n",
    "        position = torch.arange(max_seq_len).unsqueeze(1)\n",
    "        \n",
    "        exp_input = torch.arange(0, d_model, 2) * (-math.log(10000.0) / d_model)\n",
    "        \n",
    "        div_term = torch.exp(exp_input) # Returns a new tensor with the exponential of the elements of exp_input\n",
    "        \n",
    "        pe = torch.zeros(max_seq_len, d_model)\n",
    "\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        \n",
    "        pe[:, 1::2] = torch.cos(position * div_term) # torch.Size([target_seq_len, dim_val])\n",
    "\n",
    "        pe = pe.unsqueeze(0).transpose(0, 1) # torch.Size([target_seq_len, input_size, dim_val])\n",
    "\n",
    "        # register that pe is not a model parameter\n",
    "        self.register_buffer('pe', pe)\n",
    "        \n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: Tensor, shape [batch_size, enc_seq_len, dim_val]\n",
    "        \"\"\"\n",
    "\n",
    "        add = self.pe[:x.size(1), :].squeeze(1)\n",
    "\n",
    "        x = x + add\n",
    "\n",
    "        return self.dropout(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3e648525-fd6a-44a6-b238-f425d4285835",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TimeSeriesTransformer(nn.Module):\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        input_size, \n",
    "        dec_seq_len,\n",
    "        out_seq_len,\n",
    "        max_seq_len,\n",
    "        dim_val,\n",
    "        n_encoder_layers = 4,\n",
    "        n_decoder_layers = 4,\n",
    "        dropout_encoder: float=0.2, \n",
    "        dropout_decoder: float=0.2,\n",
    "        dropout_pos_enc: float=0.2,\n",
    "        dim_feedforward_encoder: int=2048,\n",
    "        dim_feedforward_decoder: int=2048,\n",
    "        n_heads = 8,\n",
    "    ):\n",
    "        \n",
    "        super().__init__()\n",
    "        self.dec_seq_len = dec_seq_len\n",
    "\n",
    "        print(\"input_size is: {}\".format(input_size))\n",
    "        print(\"dim_val is: {}\".format(dim_val))\n",
    "        \n",
    "        self.encoder_input_layers = nn.Linear(in_features=input_size, out_features=dim_val)\n",
    "        self.decoder_input_layer = nn.Linear(in_features=input_size, out_features=dim_val)\n",
    "        \n",
    "        \n",
    "        self.positional_encoder = PositionalEncoder(dropout=dropout_pos_enc, max_seq_len=max_seq_len, d_model=dim_val)\n",
    "        \n",
    "        encoder_layers = nn.TransformerEncoderLayer(d_model=dim_val, nhead=n_heads, dropout=dropout_encoder, dim_feedforward = dim_feedforward_encoder)\n",
    "        self.encoder = nn.TransformerEncoder(encoder_layer=encoder_layers, num_layers=n_encoder_layers)\n",
    "        \n",
    "        decoder_layer = nn.TransformerDecoderLayer(d_model=dim_val, nhead=n_heads, dropout=dropout_decoder, dim_feedforward = dim_feedforward_decoder)\n",
    "        self.decoder = nn.TransformerDecoder(decoder_layer=decoder_layer, num_layers=n_decoder_layers)\n",
    "        \n",
    "        self.decoder_linear_mapping = nn.Linear(in_features=out_seq_len * dim_val, out_features=out_seq_len)\n",
    "        \n",
    "        \n",
    "        \n",
    "    def forward(self, src, tgt, src_mask, tgt_mask):\n",
    "        \n",
    "        src = self.encoder_input_layers(src)\n",
    "        src = self.positional_encoder(src)\n",
    "        \n",
    "        src = self.encoder(src = src)\n",
    "        \n",
    "        \n",
    "        tgt = self.decoder_input_layer(tgt)\n",
    "        tgt = self.decoder(tgt = tgt, tgt_mask = tgt_mask, memory = src, memory_mask = src_mask)\n",
    "        \n",
    "        \n",
    "        decoder_output = self.decoder_linear_mapping(tgt)\n",
    "        return decoder_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19e182c4-b370-4fb5-a59f-55dbf0ee78c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = 1 # The number of features we want to use\n",
    "dec_seq_len = 30\n",
    "enc_seq_len = 150\n",
    "\n",
    "out_seq_len = 1\n",
    "dim_val = 512\n",
    "\n",
    "max_seq_len\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:general]",
   "language": "python",
   "name": "conda-env-general-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
